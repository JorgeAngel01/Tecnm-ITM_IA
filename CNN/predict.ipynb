{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "#from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense, Conv2D\n",
    ")\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('/home/angel/Desktop/IA/Tecnm-ITM_IA/CNN/flor_model.h5py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "\n",
    "input_image_path = \"../assets/flowers/tulipacf.jpg\"\n",
    "output_image_path = \"../assets/flowers/resized_tulipacf.jpg\"\n",
    "\n",
    "target_size = (32, 32)\n",
    "\n",
    "image = io.imread(input_image_path)\n",
    "\n",
    "# Convert the image array to float and normalize to the range [0, 1]\n",
    "image = image.astype(np.float32) / 255.0\n",
    "\n",
    "# Resize the image\n",
    "resized_image = transform.resize(image, target_size, anti_aliasing=True)\n",
    "\n",
    "# Convert the resized image back to uint8\n",
    "resized_image = (resized_image * 255).astype(np.uint8)\n",
    "\n",
    "io.imsave(output_image_path, resized_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 144ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Found array with shape: (1, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/angel/Desktop/IA/Tecnm-ITM_IA/CNN/predict.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/angel/Desktop/IA/Tecnm-ITM_IA/CNN/predict.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(img_array)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/angel/Desktop/IA/Tecnm-ITM_IA/CNN/predict.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Decode and print the top-3 predicted classes\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/angel/Desktop/IA/Tecnm-ITM_IA/CNN/predict.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m decoded_predictions \u001b[39m=\u001b[39m decode_predictions(predictions, top\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/angel/Desktop/IA/Tecnm-ITM_IA/CNN/predict.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (imagenet_id, label, score) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(decoded_predictions):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/angel/Desktop/IA/Tecnm-ITM_IA/CNN/predict.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/IA/Tecnm-ITM_IA/env/lib/python3.10/site-packages/keras/src/applications/inception_v3.py:455\u001b[0m, in \u001b[0;36mdecode_predictions\u001b[0;34m(preds, top)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.applications.inception_v3.decode_predictions\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode_predictions\u001b[39m(preds, top\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m imagenet_utils\u001b[39m.\u001b[39;49mdecode_predictions(preds, top\u001b[39m=\u001b[39;49mtop)\n",
      "File \u001b[0;32m~/Desktop/IA/Tecnm-ITM_IA/env/lib/python3.10/site-packages/keras/src/applications/imagenet_utils.py:154\u001b[0m, in \u001b[0;36mdecode_predictions\u001b[0;34m(preds, top)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mglobal\u001b[39;00m CLASS_INDEX\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(preds\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m preds\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`decode_predictions` expects \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39ma batch of predictions \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(i.e. a 2D array of shape (samples, 1000)). \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with shape: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(preds\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[39mif\u001b[39;00m CLASS_INDEX \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     fpath \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39mget_file(\n\u001b[1;32m    162\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimagenet_class_index.json\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    163\u001b[0m         CLASS_INDEX_PATH,\n\u001b[1;32m    164\u001b[0m         cache_subdir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    165\u001b[0m         file_hash\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mc2c37ea517e94d9795004a39431a14cb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    166\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Found array with shape: (1, 5)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "target_size = (32, 32)\n",
    "tulip = \"../assets/tulipacf.jpg\"\n",
    "\n",
    "# Load the image and resize it to match the model's input size\n",
    "img = image.load_img(tulip, target_size=target_size)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array = preprocess_input(img_array)\n",
    "\n",
    "# Make prediction\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Decode and print the top-3 predicted classes\n",
    "decoded_predictions = decode_predictions(predictions, top=3)[0]\n",
    "for i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n",
    "    print(f\"{i + 1}: {label} ({score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "Predicted class: common_daisy\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" model_path = '/home/angel/Desktop/IA/Tecnm-ITM_IA/CNN/flor_model.h5py' \"\"\"\n",
    "model_path = './flor_model5.h5py'\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "target_size = (64, 64)\n",
    "flower = \"../assets/flowers/cd1.jpg\"\n",
    "\n",
    "# Load the image and resize it to match the model's input size\n",
    "img = image.load_img(flower, target_size=target_size)\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Make prediction\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Map the predicted indices to class labels\n",
    "class_labels = [\"astilbe\",\"black_eyed_susan\", \"common_daisy\", \"iris\", \"paradisebirds\"]\n",
    "predicted_class_indices = np.argmax(predictions, axis=1)\n",
    "predicted_labels = [class_labels[i] for i in predicted_class_indices]\n",
    "\n",
    "# Print the predicted class labels\n",
    "print(\"Predicted class:\", predicted_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
